{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.8:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1575238661181)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4091911\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, DataFrameNaFunctions, SparkSession}\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, IDF, RegexTokenizer, StopWordsRemover, Tokenizer, StringIndexer, OneHotEncoderEstimator}\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, DecisionTreeClassifier}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.sql.functions.udf\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, DataFrameNaFunctions, SparkSession}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, IDF,  RegexTokenizer, StopWordsRemover, Tokenizer, StringIndexer, OneHotEncoderEstimator}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, DecisionTreeClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.functions.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4091911\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "      .builder\n",
    "      .appName(\"TP Spark : Trainer\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfClean: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfClean: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .parquet(\"data/train_clean_PF.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_247060d34965\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer()\n",
    "      .setPattern(\"\\\\W+\")\n",
    "      .setGaps(true)\n",
    "      .setInputCol(\"text\")\n",
    "      .setOutputCol(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_561c132b0fae\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val remover = new StopWordsRemover()\n",
    "      .setInputCol(\"tokens\")\n",
    "      .setOutputCol(\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenized: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 10 more fields]\n",
       "wordsData: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 11 more fields]\n",
       "cvModel: org.apache.spark.ml.feature.CountVectorizer = cntVec_de41d9a1221b\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenized: DataFrame = tokenizer.transform(dfClean) //Transformation des phrases en liste de mots\n",
    "val wordsData: DataFrame = remover.transform(tokenized) // Suppression des StopWords\n",
    "\n",
    "val cvModel = new CountVectorizer() // Extracts a vocabulary from document collections and generates a CountVectorizerModel.\n",
    "      .setInputCol(\"filtered\")\n",
    "      .setOutputCol(\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featurizedData: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_ecceefed36f6\n",
       "idfModel: org.apache.spark.ml.feature.IDFModel = idf_ecceefed36f6\n",
       "rescaledData: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 13 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featurizedData: DataFrame = cvModel.fit(wordsData).transform(wordsData) //application du cvModel\n",
    "\n",
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"tfidf\") //IDF is an Estimator which is fit on a dataset and produces an IDFModel.\n",
    "\n",
    "val idfModel = idf.fit(featurizedData)\n",
    "\n",
    "val rescaledData: DataFrame = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexerCountry: org.apache.spark.ml.feature.StringIndexer = strIdx_4b7031eb70e5\n",
       "indexerCurrency: org.apache.spark.ml.feature.StringIndexer = strIdx_9a15fe945938\n",
       "indexedCountry: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 14 more fields]\n",
       "indexedData: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 15 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexerCountry = new StringIndexer()\n",
    "      .setInputCol(\"country2\")\n",
    "      .setOutputCol(\"country_indexed\")\n",
    "      .setHandleInvalid(\"keep\")\n",
    "\n",
    "\n",
    "val indexerCurrency = new StringIndexer()\n",
    "      .setInputCol(\"currency2\")\n",
    "      .setOutputCol(\"currency_indexed\")\n",
    "      .setHandleInvalid(\"keep\")\n",
    "\n",
    "val indexedCountry: DataFrame = indexerCountry.fit(rescaledData).transform(rescaledData)\n",
    "\n",
    "val indexedData: DataFrame = indexerCurrency.fit(indexedCountry).transform(indexedCountry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_51b4afe65648\n",
       "encoded: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 17 more fields]\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_f84a96dd9b5f\n",
       "output: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 18 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder = new OneHotEncoderEstimator()\n",
    "      .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "      .setOutputCols(Array(\"country_onehot\", \"currency_onehot\"))\n",
    "\n",
    "\n",
    "val encoded = encoder.fit(indexedData).transform(indexedData)\n",
    "//encoded.show()\n",
    "\n",
    "//Mettre les données sous une forme utilisable par Spark.ML\n",
    "val assembler = new VectorAssembler()\n",
    "      .setInputCols(Array(\"tfidf\", \"days_campaign\", \"hours_prepa\", \"goal\", \"country_onehot\", \"currency_onehot\"))\n",
    "      .setOutputCol(\"features\")\n",
    "\n",
    "val output = assembler.transform(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_cbe94050ea63\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val lr = new LogisticRegression()\n",
    "       //.setElasticNetParam(0.0)\n",
    "       .setFitIntercept(true)\n",
    "       .setFeaturesCol(\"features\")\n",
    "       .setLabelCol(\"final_status\")\n",
    "       .setStandardization(true)\n",
    "       .setPredictionCol(\"predictions\")\n",
    "       .setRawPredictionCol(\"raw_predictions\")\n",
    "       .setThresholds(Array(0.7, 0.3))\n",
    "       .setTol(1.0e-6)\n",
    "       .setMaxIter(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_1fb7e32bc46f\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 9 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Création du Pipeline\n",
    "val pipeline = new Pipeline()\n",
    "        .setStages(Array(tokenizer, remover, cvModel, idf, indexerCountry, indexerCurrency, encoder, assembler, lr))\n",
    "\n",
    "//Entraînement, test, et sauvegarde du modèle\n",
    "val Array(training, test) = dfClean.randomSplit(Array(0.9, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_1fb7e32bc46f\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Entraînement du modèle\n",
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfWithSimplePredictions: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 21 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithSimplePredictions = model.transform(test)\n",
    "//dfWithSimplePredictions.groupBy(\"final_status\", \"predictions\").count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.3873168033920177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_360979ebf9cd\n",
       "accuracy: Double = 0.6126831966079823\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(dfWithSimplePredictions)\n",
    "println(\"Test Error = \" + (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tcntVec_de41d9a1221b-minDF: 55.0,\n",
       "\tlogreg_cbe94050ea63-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_de41d9a1221b-minDF: 75.0,\n",
       "\tlogreg_cbe94050ea63-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_de41d9a1221b-minDF: 95.0,\n",
       "\tlogreg_cbe94050ea63-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_de41d9a1221b-minDF: 55.0,\n",
       "\tlogreg_cbe94050ea63-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_de41d9a1221b-minDF: 75.0,\n",
       "\tlogreg_cbe94050ea63-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_de41d9a1221b-minDF: 95.0,\n",
       "\tlogreg_cbe94050ea63-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_de41d9a1221b-minDF: 55.0,\n",
       "\tlogreg_cbe94050ea63-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_de41d9a1221b-minDF: 75.0,\n",
       "\tlogreg_cbe94050ea63-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_de41d9a1221b-minDF: 95.0,\n",
       "\tlogreg_cbe94050ea63-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_de41d9a1221b-minDF: 55.0,\n",
       "\tlogreg_cb..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Réglage des hyper-paramètres (a.k.a. tuning) du modèle\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "      .addGrid(cvModel.minDF, (55.0 to 95.0 by 20.0).toArray)\n",
    "      .addGrid(lr.regParam, Array(10e-8, 10e-6, 10e-4,10e-2))\n",
    "      .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_e54ebdc42a3a\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "      .setEstimator(pipeline)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      // 80% of the data will be used for training and the remaining 20% for validation.\n",
    "      .setTrainRatio(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 1049|\n",
      "|           0|        0.0| 4426|\n",
      "|           1|        1.0| 2414|\n",
      "|           0|        1.0| 2960|\n",
      "+------------+-----------+-----+\n",
      "\n",
      "Test Error = 0.3695271453590193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_tvs: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_e54ebdc42a3a\n",
       "dfWithPredictions: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 21 more fields]\n",
       "accuracy_tvs: Double = 0.6304728546409807\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val  model_tvs = trainValidationSplit.fit(training)\n",
    "val dfWithPredictions = model_tvs.transform(test)\n",
    "    dfWithPredictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "val accuracy_tvs = evaluator.evaluate(dfWithPredictions)\n",
    "println(\"Test Error = \" + (1.0 - accuracy_tvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplément test de l' algorithme Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_52af6d6a3946\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dt = new  DecisionTreeClassifier()\n",
    "       .setFeaturesCol(\"features\")\n",
    "       .setLabelCol(\"final_status\")\n",
    "       .setPredictionCol(\"predictions\")\n",
    "       .setRawPredictionCol(\"raw_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_dt: org.apache.spark.ml.Pipeline = pipeline_ee2ff7411c52\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_dt = new Pipeline()\n",
    "        .setStages(Array(tokenizer, remover, cvModel, idf, indexerCountry, indexerCurrency, encoder, assembler, dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bc_model: org.apache.spark.ml.PipelineModel = pipeline_ee2ff7411c52\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bc_model = pipeline_dt.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfWithSimplePredictionsDt: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 21 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithSimplePredictionsDt = bc_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator_dt: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_5c5636da3bca\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator_dt = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.3145912065628168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_dt: Double = 0.6854087934371832\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy_dt = evaluator_dt.evaluate(dfWithSimplePredictionsDt)\n",
    "println(\"Test Error = \" + (1.0 - accuracy_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tdtc_52af6d6a3946-maxBins: 2,\n",
       "\tdtc_52af6d6a3946-maxDepth: 1\n",
       "}, {\n",
       "\tdtc_52af6d6a3946-maxBins: 10,\n",
       "\tdtc_52af6d6a3946-maxDepth: 1\n",
       "}, {\n",
       "\tdtc_52af6d6a3946-maxBins: 20,\n",
       "\tdtc_52af6d6a3946-maxDepth: 1\n",
       "}, {\n",
       "\tdtc_52af6d6a3946-maxBins: 40,\n",
       "\tdtc_52af6d6a3946-maxDepth: 1\n",
       "}, {\n",
       "\tdtc_52af6d6a3946-maxBins: 60,\n",
       "\tdtc_52af6d6a3946-maxDepth: 1\n",
       "}, {\n",
       "\tdtc_52af6d6a3946-maxBins: 80,\n",
       "\tdtc_52af6d6a3946-maxDepth: 1\n",
       "}, {\n",
       "\tdtc_52af6d6a3946-maxBins: 100,\n",
       "\tdtc_52af6d6a3946-maxDepth: 1\n",
       "}, {\n",
       "\tdtc_52af6d6a3946-maxBins: 2,\n",
       "\tdtc_52af6d6a3946-maxDepth: 2\n",
       "}, {\n",
       "\tdtc_52af6d6a3946-maxBins: 10,\n",
       "\tdtc_52af6d6a3946-maxDepth: 2\n",
       "}, {\n",
       "\tdtc_52af6d6a3946-maxBins: 20,\n",
       "\tdtc_52af6d6a3946-maxDepth: 2\n",
       "}, {\n",
       "\tdtc_52af6d6a3946-maxBins: 40,\n",
       "\tdtc_52af6d6a3946-maxDepth: 2\n",
       "}, {\n",
       "\tdtc_52..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dtparamGrid = new ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth,(1 to 11 by 1).toArray)\n",
    "             .addGrid(dt.maxBins, Array(2,10,20,40,60,80,100))\n",
    "             .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt_tvs: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_86d045603396\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dt_tvs = new TrainValidationSplit()\n",
    "      .setEstimator(pipeline_dt)\n",
    "      .setEvaluator(evaluator_dt)\n",
    "      .setEstimatorParamMaps(dtparamGrid)\n",
    "      .setTrainRatio(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val  model_dt_tvs = dt_tvs.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfWithPredictionsDt = model_dt_tvs.transform(test)\n",
    "//dfWithPredictionsDt.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "val accuracy_dt_tvs = evaluator_dt.evaluate(dfWithPredictionsDt)\n",
    "println(\"Test Error = \" + (1.0 - accuracy_dt_tvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_model.write.overwrite.save(\"models/decision_tree_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tvs.write.overwrite.save(\"models/model_lr_tvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
